{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14c3b168-15c6-486b-91d0-d0adfa5bee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f0fe8f1-b0fc-49c7-aa10-cd207375fb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2c96b81-2429-4203-9067-97300e2e66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7541419-45e2-463b-bad4-2653fe15844e",
   "metadata": {},
   "source": [
    "### Gready Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "872135f2-b450-4d6b-8914-a2ef52700c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "greedy_output = model.generate(input_ids, max_length=50, min_length=10)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae42746-20ae-4fa2-90ad-c4c3aa81a804",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12a5c28d-05f9-4872-b938-9c163bcaa399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to\n",
      "1: I enjoy walking with my cute dog, but I don't think I'll ever be able to walk\n",
      "2: I enjoy walking with my cute dog, but I don't think I'll ever be able to do\n",
      "3: I enjoy walking with my cute dog, but I don't think I'll be able to walk with\n",
      "4: I enjoy walking with my cute dog, but I don't think I'll ever be able to get\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids, \n",
    "    max_length=20, \n",
    "    num_beams=5, \n",
    "    early_stopping=True, \n",
    "    no_repeat_ngram_size=2, \n",
    "    num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536e0ca-d078-4c59-a109-6e40736c4a2a",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcc6cde0-436f-47e8-bdc6-64743a4fdfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I don't like to be at home too much. I also find it a bit weird when I'm out shopping. I am always away from my house a lot, but I do have a few friends\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0, \n",
    "    temperature=.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa653e25-119d-436a-ba17-455c9b402059",
   "metadata": {},
   "source": [
    "### In this part, we use the top-k scheme for probability limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba6314ab-ee4a-46e3-a422-d8b5a62cd1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog. It's so good to have an environment where your dog is available to share with you and we'll be taking care of you.\n",
      "\n",
      "We hope you'll find this story interesting!\n",
      "\n",
      "I am from\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd3608-4310-4ccd-91ef-353104c4ce83",
   "metadata": {},
   "source": [
    "### top-p sampling to account for the differences in sampling distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e673ede0-aa11-49dc-88c7-38a6fdd5c07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog. He will never be the same. I watch him play.\n",
      "\n",
      "\n",
      "Guys, my dog needs a name. Especially if he is found with wings.\n",
      "\n",
      "\n",
      "What was that? I had a lot of\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5b38c3b-b54a-4588-a56b-d4c160a709db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog. It's so good to have the chance to walk with a dog. But I have this problem with the dog and how he's always looking at us and always trying to make me see that I can do something\n",
      "1: I enjoy walking with my cute dog. I like seeing him, I don't like having my dog go through me, but when we walk together that makes for a wonderful bonding moment. I appreciate the interaction, I just don't understand how it would\n",
      "2: I enjoy walking with my cute dog and playing with our kids,\" said David J. Smith, director of the Humane Society of the US.\n",
      "\n",
      "\"So as a result, I've got more work in my time,\" he said.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1936df-52b5-4d2e-be14-6f616a454a43",
   "metadata": {},
   "source": [
    "### Using a constrained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74772629-78c1-4ea0-8807-735257c27802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e086830f-9f2c-430e-b216-7d3709dda6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eb7dee79-74a4-4e9d-a1a4-ad997031f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src: https://huggingface.co/transformers/v4.1.1/_modules/transformers/generation_logits_process.html\n",
    "\n",
    "def set_scores_to_inf_for_banned_tokens(scores, banned_tokens):\n",
    "    \"\"\"\n",
    "    Modifies the scores in place by setting the banned token positions to `-inf`. Banned token is expected to be a\n",
    "    list of list of banned tokens to ban in the format [[batch index, vocabulary position],...\n",
    "\n",
    "    Args:\n",
    "        scores: logits distribution of shape (batch size, vocabulary size)\n",
    "        banned_tokens: list of list of tokens to ban of length (batch_size)\n",
    "    \"\"\"\n",
    "    banned_mask_list = []\n",
    "    for idx, batch_banned_tokens in enumerate(banned_tokens):\n",
    "        for token in batch_banned_tokens:\n",
    "            banned_mask_list.append([idx, token])\n",
    "    if not banned_mask_list:\n",
    "        return scores\n",
    "\n",
    "    banned_mask = torch.LongTensor(banned_mask_list)\n",
    "    # print(banned_mask)\n",
    "    indices = torch.ones(len(banned_mask))\n",
    "    # print(indices)\n",
    "\n",
    "    banned_mask = (\n",
    "        torch.sparse.LongTensor(banned_mask.t(), indices, scores.size()).to(scores.device).to_dense().bool()\n",
    "    )\n",
    "    scores = scores.masked_fill(banned_mask, -float(\"inf\"))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0c72ae89-a2a4-450e-a839-4ed6e0b57422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e40558b0-d7a3-482e-948b-e25edf2ab2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Word</th>\n",
       "      <th>Word in English</th>\n",
       "      <th>Happiness Score</th>\n",
       "      <th>Standard Deviation of Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>laughter</td>\n",
       "      <td>laughter</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>8.42</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank       Word Word in English  Happiness Score  \\\n",
       "0     0   laughter        laughter             8.50   \n",
       "1     1  happiness       happiness             8.44   \n",
       "2     2       love            love             8.42   \n",
       "\n",
       "   Standard Deviation of Ratings  \n",
       "0                           0.93  \n",
       "1                           0.97  \n",
       "2                           1.11  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hed_df = pd.read_csv('../datasets/Hedonometer(1).csv')\n",
    "hed_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0c1be7bb-a458-4fc8-a83e-133d04736034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10187.000000\n",
       "mean         5.373276\n",
       "std          1.092155\n",
       "min          1.300000\n",
       "25%          4.900000\n",
       "50%          5.440000\n",
       "75%          6.020000\n",
       "max          8.500000\n",
       "Name: Happiness Score, dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hed_df['Happiness Score'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4412e987-a980-4e67-baa8-142a74c7b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "import numpy as np\n",
    "\n",
    "class ABCLogits(LogitsProcessor):\n",
    "  def __init__(self, vocab):\n",
    "    \"\"\"\n",
    "    vocab is a dictionary where the keys are tokens\n",
    "    and the values are the corresponding ids.\n",
    "    \"\"\"\n",
    "    # create an array of tokens\n",
    "    # remove the 'Ġ' token (used to represent a blank space in the tokenizer)\n",
    "    self.keys = list(tokenizer.vocab.keys())\n",
    "    index_to_pop = self.keys.index('Ġ') \n",
    "    self.keys.pop(index_to_pop)\n",
    "    self.keys = np.array(self.keys)\n",
    "\n",
    "    # create an array of ids\n",
    "    # also remove the 'Ġ' token\n",
    "    self.values = list(tokenizer.vocab.values())\n",
    "    self.values.pop(index_to_pop)\n",
    "    self.values = np.array(self.values)\n",
    "\n",
    "    happy_words = hed_df[hed_df['Happiness Score'] > 4]['Word in English'].tolist()\n",
    "    all_words = hed_df['Word in English'].tolist()\n",
    "    \n",
    "    is_happy = lambda x: (x in happy_words) or (x not in all_words)\n",
    "    self.is_happy = np.vectorize(is_happy)\n",
    "    # vectorized function used to get the first character of a token\n",
    "    # ignores leading whitespaces and 'Ġ' tokens\n",
    "\n",
    "    happy_indices = np.where(self.is_happy(self.keys) == True)\n",
    "    # print(happy_indices)\n",
    "    \n",
    "    self.happy_words_values = self.values[happy_indices]\n",
    "    print()\n",
    "\n",
    "  def __call__(self, input_ids, scores):\n",
    "    # print(scores)\n",
    "    # print(np.mean(scores), np.min(scores), np.max(scores))\n",
    "    banned_tokens = []\n",
    "    banned_tokens.append(self.happy_words_values)\n",
    "    print('Number of banned words: ', colored(len(self.happy_words_values), 'red'))\n",
    "    # for every beam (partially generated sentence)\n",
    "    # for beam_index, (beam_input_ids, beam_scores) in enumerate(zip(input_ids, scores)):\n",
    "    #     if \n",
    "    #   # get the last token of this beam\n",
    "    #   last_word = tokenizer.decode(beam_input_ids[-1])\n",
    "    #   # get the first character of this last token\n",
    "    #   starting_char = self.first_char(last_word)\n",
    "    #   # if the last token starts with 'a',\n",
    "    #   # ban all words that do not start with 'b', etc.\n",
    "    #   if starting_char == 'a':\n",
    "    #     banned_tokens.append(self.not_b_values)\n",
    "    #   elif starting_char == 'b':\n",
    "    #     banned_tokens.append(self.not_c_values)\n",
    "    #   elif starting_char == 'c':\n",
    "    #     banned_tokens.append(self.not_a_values)\n",
    "    #   else:\n",
    "    #     banned_tokens.append(self.not_a_values)\n",
    "    # set the scores of all banned tokens over the beams to -inf\n",
    "    scores = set_scores_to_inf_for_banned_tokens(scores, banned_tokens)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ecffeac4-5152-4f92-a6e8-33bbaaff017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of banned words:  \u001b[31m50033\u001b[0m\n",
      "Number of banned words:  \u001b[31m50033\u001b[0m\n",
      "Number of banned words:  \u001b[31m50033\u001b[0m\n",
      "Number of banned words:  \u001b[31m50033\u001b[0m\n",
      "Number of banned words:  \u001b[31m50033\u001b[0m\n",
      "Number of banned words:  \u001b[31m50033\u001b[0m\n",
      "Number of banned words:  \u001b[31m50033\u001b[0m\n",
      "Number of banned words:  \u001b[31m50033\u001b[0m\n",
      "Number of banned words:  \u001b[31m50033\u001b[0m\n",
      "beam 0: studying isnot the only thing that can be done to\n",
      "beam 1: studying isnot the only thing that can be done.\n",
      "beam 2: studying isnot the only thing that can be done in\n",
      "beam 3: studying isnot an easy task, but it can be\n",
      "beam 4: studying isnot an easy task, but it is a\n",
      "beam 5: studying isnot the only thing that can be done with\n",
      "beam 6: studying isnot the only thing that can be done,\n",
      "beam 7: studying isnot the only thing that can be done for\n",
      "beam 8: studying isnot an easy task, but it is an\n",
      "beam 9: studying isnot the only thing that can be done by\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria\n",
    ")\n",
    "import torch\n",
    "\n",
    "# how many beams to track during the Viterbi algorithm\n",
    "num_beams = 10\n",
    "# how many beams to return after the algorithm\n",
    "num_return_beams = 10\n",
    "\n",
    "# the prompt to continue\n",
    "prompt = 'studying is'\n",
    "\n",
    "# tokenizing the prompt\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "# instantiating a BeamSearchScorer\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# instantiating a list of LogitsProcessor instances\n",
    "# using our custom ABCLogits class\n",
    "logits_processor = LogitsProcessorList([ABCLogits(tokenizer.vocab)])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=12)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  print(f'beam {index}: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285441f-7614-446b-85bf-9042d755581c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea6a57-ecf8-497e-af2e-0266344b1745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-2.0]",
   "language": "python",
   "name": "conda-env-tf-2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

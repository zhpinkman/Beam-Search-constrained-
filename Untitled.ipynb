{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00ac2ab3-0c9a-4e3e-a05d-15f0b2940f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/miniforge3/envs/tf-2.0/lib/python3.9/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "import pandas as pd\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '../dialogue_summarization/transcriptsforkeyphraseextraction/2021-07-12 14.35.38 Interscriber Wrapup.m4a.csv')\n",
    "text = '; '.join(df[df['Speaker'] == 'Mark']['Utterance'].tolist())\n",
    "inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37af0efe-4774-41cd-a768-e6128adfc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(tokenizer.decoder.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3fd92ae-1de9-4f87-9583-87cabc248e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "new_words = nltk_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53515dae-f437-456a-b168-b8cbcb13e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [word for word in values if word not in new_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4200d1eb-0043-481d-94e6-812d53621689",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_ids = [tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ffd73400-5a69-4b06-95a7-5d81392e5ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Interscriber is back up and running after a two-week break. The company is working on a new way of writing summaries of meetings. The idea is that at the end, we all sit down and write a very brief summary of the meeting with the most important point so that we get an impression of what should be in the summary.']\n"
     ]
    }
   ],
   "source": [
    "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=600, early_stopping=True, bad_words_ids=bad_words_ids)\n",
    "\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fafd81fe-e84f-4edf-ba06-1b08b5a22e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Interscriber is back up and running after a two-week break. The company is working on a new way of writing summaries of meetings. The idea is that at the end, we all sit down and write a very brief summary of the meeting with the most important point so that we get an impression of what should be in the summary.']\n"
     ]
    }
   ],
   "source": [
    "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=600, early_stopping=True)\n",
    "\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304423e9-9525-4a19-a3da-ef930e6bed8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-2.0]",
   "language": "python",
   "name": "conda-env-tf-2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
